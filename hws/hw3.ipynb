{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmCHf7bnfuyy"
   },
   "source": [
    "# HW3\n",
    "\n",
    "*deadline*: Update 08.12.2021 07:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7NmjinvfxF0"
   },
   "source": [
    "Using [transformers](https://github.com/huggingface/transformers) library solve the task [DaNetQA](https://russiansuperglue.com/tasks/task_info/DaNetQA). DaNetQA is a question answering dataset for yes/no questions. These questions are naturally occurring -- they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The text-pair classification setup is similar to existing natural language inference tasks.\n",
    "\n",
    "In this homework you should explore DaNetQA task; make a simple classifier with BERT embeddings to predict if the answer is yes or no; finetune BERT-based model on DaNetQA task; make LM model and again finetune on the classification task; compare the results of classifiers and describe your results.\n",
    "\n",
    "For this homework you can choose DaNetQA dataset or its english analogue [BoolQ](https://github.com/google-research-datasets/boolean-questions) and do the same task for English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_sdVXXOguAf"
   },
   "source": [
    "### Rules\n",
    "\n",
    "* Homework is submitted through github classroom\n",
    "* Homework should be done in a group of up to 3 people. \n",
    "If you are doing a task in a group, please indicate in system the logins/names of the group members so that they can be found.\n",
    "* Homework is made in the form of a report either in a .pdf file, or in an ipython notebook.\n",
    "* The report should contain: \n",
    "  - the numbering of tasks and items that you completed, \n",
    "  - the solution code, and \n",
    "  - a clear step-by-step description of what you did. The report should be written in an academic style, without excessive use of slang and in compliance with the norms of the Russian language.\n",
    "* Do not copy fragments of lectures, articles and Wikipedia into your report.\n",
    "* Reports consisting solely of code will not be validated and will automatically be scored at zero.\n",
    "* Plagiarism and any unfair quotation leads to zeroing of the score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooPxcyV7P8C5"
   },
   "source": [
    "#### Part 0. [1 point] Get embeddings\n",
    "\n",
    "Take any BERT model (for example any from [huggingface](https://huggingface.co/sberbank-ai)) and get BERT embeddings from the train dataset (or train + dev)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPZI5h1rguDx"
   },
   "source": [
    "#### Part 1. [2 point] Explore your data\n",
    "\n",
    "* Make a 2D reduction and draw plot for train labels.\n",
    "* Analyze your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-lcOJlQguGZ"
   },
   "source": [
    "#### Part 2. [3 points] Make first classifier\n",
    "\n",
    "* Use pretrained embeddings as features to classifier. You can use for example SVM or LinearRegression.\n",
    "* Make crossvalidation and describe your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wPYE5x6g1dd"
   },
   "source": [
    "#### Part 3. [4 points] Finetune on Classification task\n",
    "\n",
    "* Finetune on a classification task with BERT-based model you chose in the 0 part (Example of code see [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skk90DcjUzyK"
   },
   "source": [
    "#### Part 4. [4 points] Finetune LM\n",
    "* Finetune language model with BERT-based model you chose in the 0 part (Example of code see [here (you need mlm!)](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling)).\n",
    "* Draw a plot (like you did in part 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEbrqDdSVcyH"
   },
   "source": [
    "#### Part 5. [4 points] Finetune on classification task your new LM model\n",
    "\n",
    "* Finetune on a classification task with BERT-based model you've done in part 4 Example of code see [here](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtNUWBTuh2a"
   },
   "source": [
    "#### Part 6. [2 point] Results\n",
    "\n",
    "* Describe your results and experiments. Compare results from tree classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geHqcANQYGaP"
   },
   "source": [
    "**TOTALLY: you can get maximum 20 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3ERCQmUWl4h"
   },
   "source": [
    "#### EXTRA (for fun)\n",
    "\n",
    "After you have done the general part you can submit on the Russian SuperGLUE or SuperGLUE (English) leaderboards any models for your solutions of DaNetQA/BoolQ task and get the scores. You can share the results and description here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn1CchrLYtve"
   },
   "source": [
    "#### Baseline\n",
    "\n",
    "As a baseline for DaNetQA task we provide [Tf-idf solution](https://russiansuperglue.com/login/submit_info/183). We used a 20 thousand sample from Wikipedia, from Russian and English sites equally. We restricted a vocabulary to 10 thousand most common words. Then for a logistic regression was trained to predict an answer.\n",
    "\n",
    "An example of the training and usage you can find [here](https://github.com/RussianNLP/RussianSuperGLUE/blob/master/TFIDF%20baseline.ipynb) and [here](https://github.com/RussianNLP/RussianSuperGLUE/blob/master/tfidf_baseline/DaNetQA.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
